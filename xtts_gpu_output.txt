XTTS GPU Simple Test
============================================================

[1/4] Checking files...
OK: assets\reference_voice.wav

[2/4] Importing TTS...
OK: TTS imported

[3/4] Loading model to GPU...
This takes 2-3 minutes on first run
Please wait...
D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.
  warnings.warn("`gpu` will be deprecated. Please use `tts.to(device)` instead.")
 > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.
 > Using model: xtts
GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From \U0001f449v4.50\U0001f448 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
OK: Model loaded in 59.4 seconds

[4/4] Generating audio...
 > Text splitted to sentences.
['Hello!', 'This is GPU-accelerated XTTS voice cloning in action.']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
D:\Hack-051_HackX\.venv\Lib\site-packages\transformers\integrations\sdpa_attention.py:53: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
ERROR: Synthesis failed: probability tensor contains either `inf`, `nan` or element < 0
Traceback (most recent call last):
  File "D:\Hack-051_HackX\test_xtts_gpu_simple.py", line 58, in <module>
    tts.tts_to_file(
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\api.py", line 334, in tts_to_file
    wav = self.tts(
          ^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\api.py", line 276, in tts
    wav = self.synthesizer.tts(
          ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\utils\synthesizer.py", line 386, in tts
    outputs = self.tts_model.synthesize(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\tts\models\xtts.py", line 419, in synthesize
    return self.full_inference(text, speaker_wav, language, **settings)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\tts\models\xtts.py", line 488, in full_inference
    return self.inference(
           ^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\tts\models\xtts.py", line 541, in inference
    gpt_codes = self.gpt.generate(
                ^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\TTS\tts\layers\xtts\gpt.py", line 590, in generate
    gen = self.gpt_inference.generate(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\transformers\generation\utils.py", line 2223, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "D:\Hack-051_HackX\.venv\Lib\site-packages\transformers\generation\utils.py", line 3257, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
